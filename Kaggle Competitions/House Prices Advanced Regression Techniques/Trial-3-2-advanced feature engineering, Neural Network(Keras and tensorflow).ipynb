{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data_train=pd.read_csv('train.csv')\n",
    "target_data=data_train['SalePrice']\n",
    "data_train=data_train.drop('SalePrice', axis = 1)\n",
    "\n",
    "\n",
    "data_test=pd.read_csv('test.csv')\n",
    "combine=[data_train,data_test]\n",
    "data = pd.concat(combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    None\n",
      "dtype: object\n",
      "0    0.0\n",
      "dtype: float64\n",
      "0    TA\n",
      "dtype: object\n",
      "0    TA\n",
      "dtype: object\n",
      "0    No\n",
      "dtype: object\n",
      "0    Unf\n",
      "dtype: object\n",
      "0    Unf\n",
      "dtype: object\n",
      "0    SBrkr\n",
      "dtype: object\n",
      "0    Attchd\n",
      "dtype: object\n",
      "0    2005.0\n",
      "dtype: float64\n",
      "0    TA\n",
      "dtype: object\n",
      "0    Unf\n",
      "dtype: object\n",
      "0    TA\n",
      "dtype: object\n",
      "0    0.0\n",
      "dtype: float64\n",
      "0    0.0\n",
      "dtype: float64\n",
      "0    0.0\n",
      "dtype: float64\n",
      "0    0.0\n",
      "dtype: float64\n",
      "0    0.0\n",
      "dtype: float64\n",
      "0    VinylSd\n",
      "dtype: object\n",
      "0    VinylSd\n",
      "dtype: object\n",
      "0    Typ\n",
      "dtype: object\n",
      "0    0.0\n",
      "dtype: float64\n",
      "0    2.0\n",
      "dtype: float64\n",
      "0    2.0\n",
      "dtype: float64\n",
      "0    TA\n",
      "dtype: object\n",
      "0    RL\n",
      "dtype: object\n",
      "0    WD\n",
      "dtype: object\n",
      "0    0.0\n",
      "dtype: float64\n",
      "0    AllPub\n",
      "dtype: object\n",
      "['MSSubClass', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold', 'LotFrontage', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'GarageYrBlt', 'GarageCars', 'GarageArea']\n",
      "['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'SaleType', 'SaleCondition']\n"
     ]
    }
   ],
   "source": [
    "raw_data=data.drop('Id', axis = 1)\n",
    "#drop \"Alley\" as it has only 91 entries\n",
    "raw_data=raw_data.drop('Alley', axis = 1)\n",
    "# replace miss values of 'LotFrontage' with its median\n",
    "raw_data.loc[ raw_data.LotFrontage.isnull(),'LotFrontage'] =raw_data['LotFrontage'].median()\n",
    "# replace miss values of 'MasVnrType' with its mode\n",
    "print(raw_data['MasVnrType'].mode())\n",
    "raw_data.loc[ raw_data.MasVnrType.isnull(),'MasVnrType'] ='None'\n",
    "# replace miss values of 'MasVnrArea' with its mode\n",
    "print(raw_data['MasVnrArea'].mode())\n",
    "raw_data.loc[ raw_data.MasVnrArea.isnull(),'MasVnrArea'] =0.0\n",
    "# replace miss values of 'BsmtQual' with its mode\n",
    "print(raw_data['BsmtQual'].mode())\n",
    "raw_data.loc[ raw_data.BsmtQual.isnull(),'BsmtQual'] ='TA'\n",
    "# replace miss values of 'BsmtCond' with its mode\n",
    "print(raw_data['BsmtCond'].mode())\n",
    "raw_data.loc[ raw_data.BsmtCond.isnull(),'BsmtCond'] ='TA'\n",
    "# replace miss values of 'BsmtExposure' with its mode\n",
    "print(raw_data['BsmtExposure'].mode())\n",
    "raw_data.loc[ raw_data.BsmtExposure.isnull(),'BsmtExposure'] ='No'\n",
    "# replace miss values of 'BsmtFinType1' with its mode\n",
    "print(raw_data['BsmtFinType1'].mode())\n",
    "raw_data.loc[ raw_data.BsmtFinType1.isnull(),'BsmtFinType1'] ='Unf'\n",
    "# replace miss values of 'BsmtFinType2' with its mode\n",
    "print(raw_data['BsmtFinType2'].mode())\n",
    "raw_data.loc[ raw_data.BsmtFinType2.isnull(),'BsmtFinType2'] ='Unf'\n",
    "# replace miss values of 'BElectrical' with its mode\n",
    "print(raw_data['Electrical'].mode())\n",
    "raw_data.loc[ raw_data.Electrical.isnull(),'Electrical'] ='SBrkr'\n",
    "#drop \"FireplaceQu\" as it has only 770 entries\n",
    "raw_data=raw_data.drop('FireplaceQu', axis = 1)\n",
    "# replace miss values of 'GarageType' with its mode\n",
    "print(raw_data['GarageType'].mode())\n",
    "raw_data.loc[ raw_data.GarageType.isnull(),'GarageType'] ='Attchd'\n",
    "# replace miss values of 'GarageYrBlt' with its mode\n",
    "print(raw_data['GarageYrBlt'].mode())\n",
    "raw_data.loc[ raw_data.GarageYrBlt.isnull(),'GarageYrBlt'] =2005.0\n",
    "# replace miss values of 'GarageQual' with its mode\n",
    "print(raw_data['GarageQual'].mode())\n",
    "raw_data.loc[ raw_data.GarageQual.isnull(),'GarageQual'] ='TA'\n",
    "# replace miss values of 'GarageFinish' with its mode\n",
    "print(raw_data['GarageFinish'].mode())\n",
    "raw_data.loc[ raw_data.GarageFinish.isnull(),'GarageFinish'] ='Unf'\n",
    "# replace miss values of 'GarageCond' with its mode\n",
    "print(raw_data['GarageCond'].mode())\n",
    "raw_data.loc[ raw_data.GarageCond.isnull(),'GarageCond'] ='TA'\n",
    "#drop \"PoolQC \" as it has only 7 entries\n",
    "raw_data=raw_data.drop('PoolQC', axis = 1)\n",
    "#drop \"Fence \" as it has only 281 entries\n",
    "raw_data=raw_data.drop('Fence', axis = 1)\n",
    "#drop \"MiscFeature  \" as it has only 54 entries\n",
    "raw_data=raw_data.drop('MiscFeature', axis = 1)\n",
    "# replace miss values of 'BsmtFinSF1' with its mode\n",
    "print(raw_data['BsmtFinSF1'].mode())\n",
    "raw_data.loc[ raw_data.BsmtFinSF1.isnull(),'BsmtFinSF1'] =0.0\n",
    "# replace miss values of 'BsmtFinSF2' with its mode\n",
    "print(raw_data['BsmtFinSF2'].mode())\n",
    "raw_data.loc[ raw_data.BsmtFinSF2.isnull(),'BsmtFinSF2'] =0.0\n",
    "# replace miss values of 'BsmtFullBath' with its mode\n",
    "print(raw_data['BsmtFullBath'].mode())\n",
    "raw_data.loc[ raw_data.BsmtFullBath.isnull(),'BsmtFullBath'] =0.0\n",
    "# replace miss values of 'BsmtHalfBath' with its mode\n",
    "print(raw_data['BsmtHalfBath'].mode())\n",
    "raw_data.loc[ raw_data.BsmtHalfBath.isnull(),'BsmtHalfBath'] =0.0\n",
    "# replace miss values of 'BsmtUnfSF' with its mode\n",
    "print(raw_data['BsmtUnfSF'].mode())\n",
    "raw_data.loc[ raw_data.BsmtUnfSF.isnull(),'BsmtUnfSF'] =0.0\n",
    "# replace miss values of 'Exterior1st' with its mode\n",
    "print(raw_data['Exterior1st'].mode())\n",
    "raw_data.loc[ raw_data.Exterior1st.isnull(),'Exterior1st'] ='VinylSd'\n",
    "# replace miss values of 'Exterior2nd ' with its mode\n",
    "print(raw_data['Exterior2nd'].mode())\n",
    "raw_data.loc[ raw_data.Exterior2nd .isnull(),'Exterior2nd'] ='VinylSd'\n",
    "# replace miss values of 'Functional' with its mode\n",
    "print(raw_data['Functional'].mode())\n",
    "raw_data.loc[ raw_data.Functional.isnull(),'Functional'] ='Typ'\n",
    "# replace miss values of 'GarageArea' with its mode\n",
    "print(raw_data['GarageArea'].mode())\n",
    "raw_data.loc[ raw_data.GarageArea.isnull(),'GarageArea'] =0.0\n",
    "# replace miss values of 'GarageCars' with its mode\n",
    "print(raw_data['GarageCars'].mode())\n",
    "raw_data.loc[ raw_data.GarageCars.isnull(),'GarageCars'] =2.0\n",
    "# replace miss values of 'GarageCars' with its mode\n",
    "print(raw_data['GarageCars'].mode())\n",
    "raw_data.loc[ raw_data.GarageCars.isnull(),'GarageCars'] =2.0\n",
    "# replace miss values of 'KitchenQual' with its mode\n",
    "print(raw_data['KitchenQual'].mode())\n",
    "raw_data.loc[ raw_data.KitchenQual.isnull(),'KitchenQual'] ='TA'\n",
    "# replace miss values of 'MSZoning' with its mode\n",
    "print(raw_data['MSZoning'].mode())\n",
    "raw_data.loc[ raw_data.MSZoning.isnull(),'MSZoning'] ='RL'\n",
    "# replace miss values of 'SaleType' with its mode\n",
    "print(raw_data['SaleType'].mode())\n",
    "raw_data.loc[ raw_data.SaleType.isnull(),'SaleType'] ='WD'\n",
    "# replace miss values of 'TotalBsmtSF' with its mode\n",
    "print(raw_data['TotalBsmtSF'].mode())\n",
    "raw_data.loc[ raw_data.TotalBsmtSF.isnull(),'TotalBsmtSF'] =0.0\n",
    "# replace miss values of 'Utilities' with its mode\n",
    "print(raw_data['Utilities'].mode())\n",
    "raw_data.loc[ raw_data.Utilities.isnull(),'Utilities'] ='AllPub'\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "numerical = list(raw_data.select_dtypes(include=['int64']).columns.values)+list(raw_data.select_dtypes(include=['float64']).columns.values)\n",
    "print(numerical)\n",
    "raw_data[numerical] = scaler.fit_transform(raw_data[numerical])\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "categorical=list(raw_data.select_dtypes(include=['object']).columns.values)\n",
    "print(categorical)\n",
    "for value in categorical:\n",
    "    le.fit(raw_data[value])\n",
    "    raw_data[value]=le.transform(raw_data[value])\n",
    "raw_data.head()\n",
    "\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "feature_train_all=raw_data[:1460]\n",
    "feature_test=raw_data[1460:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_train_all,target_data, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Modeling-Keras and tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trial_1:benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_features(X): \n",
    "    ''' scale the features columnwise to 0 mean and normalize by range\n",
    "    '''\n",
    "    for i in range(len(X[1,:])):\n",
    "        X[:,i] = (X[:,i] - X[:,i].mean())/(X[:,i].max()-X[:,i].min())\n",
    "    return X\n",
    "\n",
    "def fill_nan(X):\n",
    "    '''replace NaNs with mean for each column\n",
    "    ''' \n",
    "    for i in range(len(X[1,:])):\n",
    "        mean = np.nanmean(X[:,i])\n",
    "        mask = np.isnan(X[:,i])\n",
    "        X[mask,i] = mean\n",
    "    return X\n",
    "\n",
    "\n",
    "def strings_to_num(df):\n",
    "    '''Input: df (Pandas dataframe)\n",
    "       Ouput: numpy array with categorical (string) columns transformed into numerical\n",
    "    '''\n",
    "    #transform dataframe categories to numbers\n",
    "    return df.apply(lambda x: pd.factorize(x)[0]).values\n",
    "\n",
    "def df_to_numpy_array(df):\n",
    "    ''' Seperate categorical and numerical columns of dataframe\n",
    "        Input df: Pandas dataframe\n",
    "        Output: numpy array\n",
    "    '''\n",
    "    #get names of numerical columns\n",
    "    num_columns = list(df.select_dtypes(include=['float64', 'int64']).columns.values)\n",
    "    \n",
    "    #get numerical values into NumPy array\n",
    "    num_values = df[num_columns].values\n",
    "    \n",
    "    #fill NaN in numerical features\n",
    "    num_values = fill_nan(num_values)\n",
    "    \n",
    "    #scale numerical features\n",
    "    num_values = scale_features(num_values)   \n",
    "    \n",
    "    #get categorical columns \n",
    "    cat_columns = list(df.select_dtypes(include=['object']).columns.values)\n",
    "\n",
    "    #transform categorical columns into numpy array\n",
    "    cat_values = strings_to_num(df[cat_columns])\n",
    "    \n",
    "    return np.concatenate((cat_values,num_values),axis=1)\n",
    "\n",
    "\n",
    "feature_names = train_df.drop([\"SalePrice\",\"Id\"],axis=1).columns.tolist()\n",
    "num_features = len(feature_names)    \n",
    "    \n",
    "X = df_to_numpy_array(train_df.drop([\"SalePrice\",\"Id\"],axis=1))     #training data\n",
    "Xtest = df_to_numpy_array(test_df.drop([\"Id\"],axis=1)) #test data\n",
    "y = train_df['SalePrice'].values #target\n",
    "test_ids = test_df[\"Id\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# define base mode\n",
    "def model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=num_features, init='normal', activation='relu'))\n",
    "    model.add(Dense(20, init='normal', activation='relu'))\n",
    "    model.add(Dense(1, init='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='msle', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# evaluate model with standardized dataset\n",
    "kr = KerasRegressor(build_fn=model, nb_epoch=100, batch_size=5, verbose=0)\n",
    "\n",
    "#scores = cross_val_score(kr, X, y, cv=4)\n",
    "#print(\"msle = %4.2f std = %4.2f\" % (scores.mean(),scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, activation=\"relu\", input_dim=79, kernel_initializer=\"normal\")`\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"normal\")`\n"
     ]
    }
   ],
   "source": [
    "kr = KerasRegressor(build_fn=model, epochs=500, batch_size=5, verbose=0)\n",
    "kr = kr.fit(X_train,y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = kr.model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.725077272525\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "print(r2_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trial_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# define base mode\n",
    "def model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=num_features, init='normal', activation='relu'))\n",
    "    model.add(Dense(50, init='normal', activation='relu'))\n",
    "    model.add(Dense(50, init='normal', activation='relu'))\n",
    "    model.add(Dense(1, init='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='msle', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# evaluate model with standardized dataset\n",
    "#kr = KerasRegressor(build_fn=model, nb_epoch=500, batch_size=1, verbose=0)\n",
    "\n",
    "#scores = cross_val_score(kr, X, y, cv=4)\n",
    "#print(\"msle = %4.2f std = %4.2f\" % (scores.mean(),scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, activation=\"relu\", input_dim=79, kernel_initializer=\"normal\")`\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"normal\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.403054996045\n"
     ]
    }
   ],
   "source": [
    "kr = KerasRegressor(build_fn=model, epochs=500, batch_size=1, verbose=0)\n",
    "kr = kr.fit(X_train,y_train) \n",
    "\n",
    "predictions = kr.model.predict(X_test)\n",
    "print(r2_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Trial_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, activation=\"relu\", input_dim=79, kernel_initializer=\"normal\")`\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"normal\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.604504041893\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# define base mode\n",
    "def model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=num_features, init='normal', activation='relu'))\n",
    "    model.add(Dense(50, init='normal', activation='relu'))\n",
    "    model.add(Dense(50, init='normal', activation='relu'))\n",
    "    model.add(Dense(1, init='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='msle', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# evaluate model with standardized dataset\n",
    "#kr = KerasRegressor(build_fn=model, nb_epoch=500, batch_size=1, verbose=0)\n",
    "\n",
    "#scores = cross_val_score(kr, X, y, cv=4)\n",
    "#print(\"msle = %4.2f std = %4.2f\" % (scores.mean(),scores.std()))\n",
    "\n",
    "kr = KerasRegressor(build_fn=model, epochs=500, batch_size=5, verbose=0)\n",
    "kr = kr.fit(X_train,y_train) \n",
    "\n",
    "predictions = kr.model.predict(X_test)\n",
    "print(r2_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Trial_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, activation=\"relu\", input_dim=79, kernel_initializer=\"normal\")`\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"normal\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.741081433274\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# define base mode\n",
    "def model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=num_features, init='normal', activation='relu'))\n",
    "    model.add(Dense(50, init='normal', activation='relu'))\n",
    "    model.add(Dense(50, init='normal', activation='relu'))\n",
    "    model.add(Dense(1, init='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='msle', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# evaluate model with standardized dataset\n",
    "#kr = KerasRegressor(build_fn=model, nb_epoch=500, batch_size=1, verbose=0)\n",
    "\n",
    "#scores = cross_val_score(kr, X, y, cv=4)\n",
    "#print(\"msle = %4.2f std = %4.2f\" % (scores.mean(),scores.std()))\n",
    "\n",
    "kr = KerasRegressor(build_fn=model, epochs=100, batch_size=5, verbose=0)\n",
    "kr = kr.fit(X_train,y_train) \n",
    "\n",
    "predictions = kr.model.predict(X_test)\n",
    "print(r2_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Trial_5 back to tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_features(X): \n",
    "    ''' scale the features columnwise to 0 mean and normalize by range\n",
    "    '''\n",
    "    for i in range(len(X[1,:])):\n",
    "        X[:,i] = (X[:,i] - X[:,i].mean())/(X[:,i].max()-X[:,i].min())\n",
    "    return X\n",
    "\n",
    "def fill_nan(X):\n",
    "    '''replace NaNs with mean for each column\n",
    "    ''' \n",
    "    for i in range(len(X[1,:])):\n",
    "        mean = np.nanmean(X[:,i])\n",
    "        mask = np.isnan(X[:,i])\n",
    "        X[mask,i] = mean\n",
    "    return X\n",
    "\n",
    "\n",
    "def strings_to_num(df):\n",
    "    '''Input: df (Pandas dataframe)\n",
    "       Ouput: numpy array with categorical (string) columns transformed into numerical\n",
    "    '''\n",
    "    #transform dataframe categories to numbers\n",
    "    return df.apply(lambda x: pd.factorize(x)[0]).values\n",
    "\n",
    "def df_to_numpy_array(df):\n",
    "    ''' Seperate categorical and numerical columns of dataframe\n",
    "        Input df: Pandas dataframe\n",
    "        Output: numpy array\n",
    "    '''\n",
    "    #get names of numerical columns\n",
    "    num_columns = list(df.select_dtypes(include=['float64', 'int64']).columns.values)\n",
    "    \n",
    "    #get numerical values into NumPy array\n",
    "    num_values = df[num_columns].values\n",
    "    \n",
    "    #fill NaN in numerical features\n",
    "    num_values = fill_nan(num_values)\n",
    "    \n",
    "    #scale numerical features\n",
    "    num_values = scale_features(num_values)   \n",
    "    \n",
    "    #get categorical columns \n",
    "    cat_columns = list(df.select_dtypes(include=['object']).columns.values)\n",
    "\n",
    "    #transform categorical columns into numpy array\n",
    "    cat_values = strings_to_num(df[cat_columns])\n",
    "    \n",
    "    return np.concatenate((cat_values,num_values),axis=1)\n",
    "\n",
    "\n",
    "feature_names = train_df.drop([\"SalePrice\",\"Id\"],axis=1).columns.tolist()\n",
    "num_features = len(feature_names) \n",
    "\n",
    "#X = train_df.drop([\"SalePrice\",\"Id\"],axis=1)     #training data\n",
    "#test data\n",
    "#y = train_df['SalePrice'].values\n",
    "\n",
    "\n",
    "X = df_to_numpy_array(train_df.drop([\"SalePrice\",\"Id\"],axis=1))     #training data\n",
    "Xtest = df_to_numpy_array(test_df.drop([\"Id\"],axis=1)) #test data\n",
    "y = train_df['SalePrice'].values #target\n",
    "test_ids = test_df[\"Id\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.401524102936\n",
      "-0.539472103761\n"
     ]
    }
   ],
   "source": [
    "training_epochs = 500\n",
    "keep_probability = 1\n",
    "learning_rate = 0.1\n",
    "layers=50\n",
    "\n",
    "\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "x = tf.placeholder(\"float\", [X_train.shape[0], X_train.shape[1]])\n",
    "weight_1=tf.Variable(tf.truncated_normal([X_train.shape[1],layers]))\n",
    "bias_1=tf.Variable(tf.zeros(layers))\n",
    "layer_1 = tf.add(tf.matmul(x, weight_1),bias_1)\n",
    "layer_1=tf.nn.relu(layer_1)\n",
    "layer_1 = tf.nn.dropout(layer_1, keep_prob)\n",
    "\n",
    "weight_2=tf.Variable(tf.truncated_normal([layer_1.get_shape().as_list()[1],layers]))\n",
    "bias_2=tf.Variable(tf.zeros(layers))\n",
    "layer_2 = tf.add(tf.matmul(layer_1, weight_2),bias_2)\n",
    "layer_2=tf.nn.relu(layer_2)\n",
    "layer_2 = tf.nn.dropout(layer_2, keep_prob)\n",
    "\n",
    "weight_3=tf.Variable(tf.truncated_normal([layer_2.get_shape().as_list()[1],layers]))\n",
    "bias_3=tf.Variable(tf.zeros(layers))\n",
    "layer_3 = tf.add(tf.matmul(layer_2, weight_3),bias_3)\n",
    "layer_3=tf.nn.relu(layer_3)\n",
    "layer_3 = tf.nn.dropout(layer_3, keep_prob)\n",
    "\n",
    "weight_4=tf.Variable(tf.truncated_normal([layer_3.get_shape().as_list()[1],1]))\n",
    "bias_4=tf.Variable(tf.zeros(1))\n",
    "output = tf.add(tf.matmul(layer_3, weight_4),bias_4)\n",
    "\n",
    "\n",
    "# Mean squared error\n",
    "y = tf.placeholder(\"float\", [y_train.shape[0]])\n",
    "cost = tf.reduce_sum(tf.pow(output-y, 2))/y_train.shape[0]\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "#predict feature train\n",
    "x_train=tf.placeholder(\"float\", [X_test.shape[0], X_test.shape[1]])\n",
    "L1 = tf.add(tf.matmul(x_train, weight_1),bias_1)\n",
    "L2=tf.add(tf.matmul(L1, weight_2),bias_2)\n",
    "L3 = tf.add(tf.matmul(L2, weight_3),bias_3)\n",
    "Pred_train= tf.add(tf.matmul(L3, weight_4),bias_4)\n",
    "\n",
    "#predict feature original\n",
    "x_original=tf.placeholder(\"float\", [X_train.shape[0], X_train.shape[1]])\n",
    "L1 = tf.add(tf.matmul(x_original, weight_1),bias_1)\n",
    "L2=tf.add(tf.matmul(L1, weight_2),bias_2)\n",
    "L3 = tf.add(tf.matmul(L2, weight_3),bias_3)\n",
    "Pred_original= tf.add(tf.matmul(L3, weight_4),bias_4)\n",
    "\n",
    "#predict feature test\n",
    "# x_test=tf.placeholder(\"float\", [1459, 74])\n",
    "# L1 = tf.add(tf.matmul(x_test, weight_1),bias_1)\n",
    "# L2=tf.add(tf.matmul(L1, weight_2),bias_2)\n",
    "# L3 = tf.add(tf.matmul(L2, weight_3),bias_3)\n",
    "# Pred_test= tf.add(tf.matmul(L3, weight_4),bias_4)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        sess.run(optimizer, feed_dict={x: X_train, y: y_train, keep_prob: keep_probability})\n",
    "    sess.run(Pred_train, feed_dict={x_train: X_test})  \n",
    "    #sess.run(Pred_test, feed_dict={x_test: feature_test})\n",
    "    sess.run(Pred_original, feed_dict={x_original: X_train})\n",
    "    #transfor Pred_train to numpy array\n",
    "    Pred_train_np = Pred_train.eval({x_train: X_test})\n",
    "    #transfor Pred_test to numpy array\n",
    "    #Pred_test_np = Pred_test.eval({x_test: feature_test})\n",
    "    #transfor Pred_original to numpy array\n",
    "    Pred_original_np = Pred_original.eval({x_original: X_train})\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "print(r2_score(y_test,Pred_train_np))\n",
    "print(r2_score(y_train,Pred_original_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
